{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34fcc05d",
   "metadata": {},
   "source": [
    "# BING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e73e1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install requests\n",
    "# !pip install bing-image-downloader\n",
    "\n",
    "from bing_image_downloader import downloader\n",
    "list = [\"gas distribution meters on site images\", \"gas distribution valves on site images\", \"gas distribution regulators on site images\", \"gas distribution piping on site images\"]\n",
    "for i in range(len(list)):\n",
    "    downloader.download(list[i], limit=50,  output_dir='bing', \n",
    "    adult_filter_off=True, force_replace=False, timeout=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f93097",
   "metadata": {},
   "source": [
    "# SELENIUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91883898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install shutil\n",
    "# !pip install selenium\n",
    "\n",
    "import os, shutil, urllib, time, requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "main_folder = 'scrapped-data'\n",
    "if not os.path.isdir(main_folder):\n",
    "    os.makedirs(main_folder)\n",
    "search = 'gas regulator on site images'\n",
    "if os.path.isdir(main_folder + '/' + search):\n",
    "    shutil.rmtree(main_folder + '/' + search)\n",
    "if not os.path.isdir(main_folder + '/' + search):\n",
    "    os.makedirs(main_folder + '/' + search)\n",
    "\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument('--headless')\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "driver = webdriver.Chrome('webdriver/chromedriver',chrome_options=chrome_options)\n",
    "url ='https://www.google.com/imghp?hl=EN'\n",
    "driver.get(url)\n",
    "img = driver.find_element(By.XPATH,\"//input[@class='gLFyf']\")    \n",
    "img.send_keys(search)\n",
    "img.send_keys(Keys.ENTER)\n",
    "src = []\n",
    "SCROLL_PAUSE_TIME = 5\n",
    "last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "while True:\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(SCROLL_PAUSE_TIME)\n",
    "    imgResults = driver.find_elements(By.XPATH,\"//img[contains(@class,'Q4LuWd')]\")\n",
    "    for img in imgResults:\n",
    "        if img.get_attribute('src')==None:\n",
    "            continue\n",
    "\n",
    "        src.append(img.get_attribute('src'))\n",
    "        print(len(src))\n",
    "        if len(src)==3000:\n",
    "            break\n",
    "            \n",
    "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    if new_height == last_height or len(src)==3000:\n",
    "        try:\n",
    "            driver.find_element(By.XPATH,\"//div[@jsname='i3y3Ic']/input\").click()    \n",
    "        except:\n",
    "            break\n",
    "    last_height = new_height\n",
    "    \n",
    "\n",
    "for i in range(len(src)):\n",
    "    try:\n",
    "        urllib.request.urlretrieve(str(src[i]),f\"{main_folder}/{search}/img{i}.jpg\")\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074b8401",
   "metadata": {},
   "source": [
    "# BEAUTIFUL SOUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24f199d",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## downloads only 80 images at once #############################################################\n",
    "# !pip install beautifulsoup4\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "google_image = \"https://www.google.com/search?site=&tbm=isch&source=hp&biw=1873&bih=990&\"\n",
    "user_agent = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Safari/537.36\"\n",
    "}\n",
    "saved_folder = 'google-80'\n",
    "\n",
    "\n",
    "def main():\n",
    "    if not os.path.exists(saved_folder):\n",
    "        os.mkdir(saved_folder)\n",
    "    list = [\"gas distribution meters on site images\", \"gas distribution valves on site images\", \"gas distribution regulators on site images\", \"gas distribution piping on site images\"]\n",
    "    for i in range(len(list)):\n",
    "        os.mkdir(saved_folder + '/' + list[i])\n",
    "        download_images(list[i])\n",
    "\n",
    "\n",
    "def download_images(items):\n",
    "#     data = input('What are you looking for? ')\n",
    "    data = items\n",
    "    n_images = 80\n",
    "    print('searching...')\n",
    "    search_url = google_image + 'q=' + data\n",
    "    response = requests.get(search_url, headers=user_agent)\n",
    "    html = response.text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    results = soup.findAll('img', {'class': 'rg_i Q4LuWd'})\n",
    "    count = 1\n",
    "    links = []\n",
    "    for result in results:\n",
    "        try:\n",
    "            link = result['data-src']\n",
    "            links.append(link)\n",
    "            count += 1\n",
    "            if(count > n_images):\n",
    "                break\n",
    "        except KeyError:\n",
    "            continue\n",
    "    print(f\"Downloading {len(links)} images...\")\n",
    "    for i, link in enumerate(links):\n",
    "        response = requests.get(link)\n",
    "        image_name = saved_folder + '/' + data + '/' + data + str(i+1) + '.jpg'\n",
    "        with open(os.path.join(image_name), 'wb') as fh:\n",
    "            fh.write(response.content)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46faf38",
   "metadata": {},
   "source": [
    "# GOOGLE LENS SEARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8ccbddac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All directories set\n",
      "Opening Browser...\n",
      "Selected \"no thanks sign in\" option\n",
      "Selected \"search by image\" option\n",
      "Uploaded Image\n",
      "Scrolling to get Attributes...\n",
      "Downloading Images...\n",
      "Images Downloaded: 59\n"
     ]
    }
   ],
   "source": [
    "import os, urllib, time, requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "def scrap(file_path, t, scroll_rate, folder_path):\n",
    "    try:\n",
    "        #click no thanks sign in button\n",
    "        iframe = driver.find_element(\"xpath\", \"//*[@id='gb']/div/div[3]/iframe\")\n",
    "        driver.switch_to.frame(iframe)\n",
    "        driver.find_element(By.XPATH, \"//div[@class ='QlyBfb']//button\").click()\n",
    "        driver.switch_to.default_content()\n",
    "        print('Selected \"no thanks sign in\" option')\n",
    "        time.sleep(t)\n",
    "\n",
    "        #click search by image button\n",
    "        cam_button = driver.find_element(\"xpath\", \"//div[@aria-label=\\\"Search by image\\\" and @role=\\\"button\\\"]\")\n",
    "        cam_button.click()\n",
    "        print('Selected \"search by image\" option')\n",
    "        time.sleep(t)\n",
    "\n",
    "        # Find upload tab and find path\n",
    "        upload_tab = driver.find_element(\"xpath\", '//div[@class=\"BH9rn\"]//div[@class=\"ZeVBtc\"]//span')\n",
    "        time.sleep(t)\n",
    "        upload_btn = driver.find_element(\"name\", \"encoded_image\")\n",
    "        upload_btn.send_keys(file_path)\n",
    "        print('Uploaded Image')\n",
    "        time.sleep(t)\n",
    "        \n",
    "        #find image attributes\n",
    "        src = []\n",
    "        img = driver.find_elements(By.XPATH, \"//div[@class='b57KQc']//div[@class='Me0cf ']//img\")\n",
    "        print('Scrolling to get Attributes...')\n",
    "\n",
    "        for i in range(len(img)):\n",
    "            driver.find_element(\"xpath\", f'(//div[contains(@class, \"Me0cf \")])[{i+1}]').location_once_scrolled_into_view\n",
    "            time.sleep(scroll_rate)\n",
    "            \n",
    "        #get attribute from src\n",
    "        for i in img:\n",
    "            if i.get_attribute('src')==None:\n",
    "                continue\n",
    "            src.append(i.get_attribute('src'))\n",
    "\n",
    "        #download images from url\n",
    "        print('Downloading Images...')\n",
    "        for j in range(len(src)):\n",
    "            try:\n",
    "                urllib.request.urlretrieve(str(src[j]),f\"{folder_path}/img{j}.jpg\")\n",
    "            except:\n",
    "                continue\n",
    "        print('Images Downloaded:', j)\n",
    "    \n",
    "    #print error message\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('Trying again, Please wait...')\n",
    "\n",
    "        #click search by image button\n",
    "        cam_button = driver.find_element(\"xpath\", \"//div[@aria-label=\\\"Search by image\\\" and @role=\\\"button\\\"]\")\n",
    "        cam_button.click()\n",
    "        print('Selected \"search by image\" option')\n",
    "        time.sleep(t)\n",
    "\n",
    "        # Find upload tab and find path\n",
    "        upload_tab = driver.find_element(\"xpath\", '//div[@class=\"BH9rn\"]//div[@class=\"ZeVBtc\"]//span')\n",
    "        time.sleep(t)\n",
    "        upload_btn = driver.find_element(\"name\", \"encoded_image\")\n",
    "        upload_btn.send_keys(file_path)\n",
    "        print('Uploaded Image')\n",
    "        time.sleep(t)\n",
    "        \n",
    "        src = []\n",
    "        img = driver.find_elements(By.XPATH, \"//div[@class='b57KQc']//div[@class='Me0cf ']//img\")\n",
    "        print('Scrolling to get Attributes...')\n",
    "\n",
    "        for i in range(len(img)):\n",
    "            driver.find_element(\"xpath\", f'(//div[contains(@class, \"Me0cf \")])[{i+1}]').location_once_scrolled_into_view\n",
    "            time.sleep(scroll_rate)\n",
    "            \n",
    "        #get attribute from src\n",
    "        for i in img:\n",
    "            if i.get_attribute('src')==None:\n",
    "                continue\n",
    "            src.append(i.get_attribute('src'))\n",
    "\n",
    "        #download images from url\n",
    "        print('Downloading Images...')\n",
    "        for j in range(len(src)):\n",
    "            try:\n",
    "                urllib.request.urlretrieve(str(src[j]),f\"{folder_path}/img{j}.jpg\")\n",
    "            except:\n",
    "                continue\n",
    "        print('Images Downloaded:', j)\n",
    "\n",
    "######################################################################################################################\n",
    "#user input\n",
    "file_path = r'C:\\Users\\seema1\\Desktop\\object-detection\\images\\a.jfif'    #path of the file to scrap images of\n",
    "t = 10    #sleep time\n",
    "scroll_rate = 0.5\n",
    "\n",
    "try:\n",
    "    #set directories\n",
    "    main_folder = 'scrapped-data-lens'\n",
    "    if not os.path.isdir(main_folder):\n",
    "        os.mkdir(main_folder)\n",
    "\n",
    "    a = file_path.split(\"\\\\\")\n",
    "    b = a[-1].split(\".\")\n",
    "    pic = b[0]\n",
    "    folder_path=main_folder + '/' + pic\n",
    "\n",
    "    count=0\n",
    "    while os.path.isdir(folder_path):\n",
    "        count+=1\n",
    "        folder_path=main_folder+'/'+pic+str(count)\n",
    "\n",
    "    os.mkdir(folder_path)\n",
    "    print('All directories set')\n",
    "\n",
    "    #driver\n",
    "#     options = webdriver.ChromeOptions()\n",
    "#     options.add_argument('--headless')\n",
    "#     options.add_argument('--no-sandbox')\n",
    "#     options.add_argument('--disable-dev-shm-usage')\n",
    "#     driver = webdriver.Chrome('webdriver/chromedriver', options=options)\n",
    "#     url ='https://www.google.com/'\n",
    "#     #url='https://images.google.com/'\n",
    "#     driver.get(url)\n",
    "    \n",
    "    \n",
    "    ser = Service(r\"webdriver/chromedriver\")\n",
    "    op = webdriver.ChromeOptions()\n",
    "    op.add_argument('--headless')\n",
    "    op.add_argument('--no-sandbox')\n",
    "    op.add_argument('--disable-dev-shm-usage')\n",
    "    driver = webdriver.Chrome(service=ser, options=op)\n",
    "    url ='https://www.google.com/'\n",
    "    driver.get(url)\n",
    "       \n",
    "    print('Opening Browser...')\n",
    "    time.sleep(t)\n",
    "\n",
    "    scrap(file_path, t, scroll_rate, folder_path)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52ef33b",
   "metadata": {},
   "source": [
    "# GOOGLE IMAGE SEARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "441986b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting URL...\n",
      "Got URL\n",
      "Setting Directories...\n",
      "All Directories Set\n",
      "Opening Browser...\n",
      "Opened Images Page. Downloading Images...\n",
      "Images Downloaded:  50\n"
     ]
    }
   ],
   "source": [
    "import os, shutil, urllib, time, requests, webbrowser, requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "filepath = r'C:\\Users\\seema1\\Desktop\\object-detection\\images\\bb.jfif'    #path of the image to scrap images of\n",
    "number_of_images = 50    #number of images to download\n",
    "delay_in_seconds = 20\n",
    "\n",
    "try:\n",
    "    #get URL\n",
    "    print('Getting URL...')\n",
    "    searchUrl = 'https://www.google.com/searchbyimage/upload'\n",
    "    multipart = {'encoded_image': (filepath, open(filepath, 'rb')), 'image_content': ''}\n",
    "    response = requests.post(searchUrl, files=multipart, allow_redirects=False)\n",
    "    fetchUrl = response.headers['Location']\n",
    "    # webbrowser.open(fetchUrl)\n",
    "    print('Got URL')\n",
    "    \n",
    "    #set directories\n",
    "    print('Setting Directories...')\n",
    "    main_folder = 'scrapped-data-images'\n",
    "    if not os.path.isdir(main_folder):\n",
    "        os.mkdir(main_folder)\n",
    "\n",
    "    a = filepath.split(\"\\\\\")\n",
    "    b = a[-1].split(\".\")\n",
    "    pic = b[0]\n",
    "    folder_path=main_folder + '/' + pic\n",
    "\n",
    "    count=0\n",
    "    while os.path.isdir(folder_path):\n",
    "        count+=1\n",
    "        folder_path=main_folder+'/'+pic+str(count)\n",
    "\n",
    "    os.mkdir(folder_path)\n",
    "    print('All Directories Set')\n",
    "\n",
    "#     #driver\n",
    "#     chrome_options = webdriver.ChromeOptions()\n",
    "#     chrome_options.add_argument('--headless')\n",
    "#     chrome_options.add_argument('--no-sandbox')\n",
    "#     chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "#     driver = webdriver.Chrome('webdriver/chromedriver',chrome_options=chrome_options)\n",
    "#     driver.get(fetchUrl)\n",
    "#     print('Opening Browser...')\n",
    "    \n",
    "    ser = Service(r\"webdriver/chromedriver\")\n",
    "    op = webdriver.ChromeOptions()\n",
    "    op.add_argument('--headless')\n",
    "    op.add_argument('--no-sandbox')\n",
    "    op.add_argument('--disable-dev-shm-usage')\n",
    "    driver = webdriver.Chrome(service=ser, options=op)\n",
    "    driver.get(fetchUrl)\n",
    "    print('Opening Browser...')\n",
    "\n",
    "    time.sleep(delay_in_seconds)\n",
    "\n",
    "    #open images page\n",
    "    elem1 = driver.find_elements(By.XPATH,'//div[@class=\"hdtb-mitem\"]//a')\n",
    "    for i in range(len(elem1)):\n",
    "        if elem1[i].text==\"All\":\n",
    "            driver.find_elements(\"xpath\", '//div[@class=\"hdtb-mitem\"]//a')[i].click()\n",
    "            break\n",
    "\n",
    "    time.sleep(delay_in_seconds)\n",
    "    elem2 = driver.find_elements(By.XPATH,'//div[@class=\"hdtb-mitem\"]//a')\n",
    "    for i in range(len(elem2)):\n",
    "        if elem2[i].text==\"Images\":\n",
    "            driver.find_elements(\"xpath\", '//div[@class=\"hdtb-mitem\"]//a')[i].click()\n",
    "            break\n",
    "\n",
    "    print('Opened Images Page. Downloading Images...')\n",
    "\n",
    "    #get attributes of images\n",
    "    src = []\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(delay_in_seconds)\n",
    "        imgResults = driver.find_elements(By.XPATH,\"//img[contains(@class,'Q4LuWd')]\")\n",
    "        for img in imgResults:\n",
    "            if img.get_attribute('src')==None:\n",
    "                continue\n",
    "\n",
    "            src.append(img.get_attribute('src'))\n",
    "            if len(src)==number_of_images:\n",
    "                break\n",
    "\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height or len(src)==number_of_images:\n",
    "            try:\n",
    "                driver.find_element(By.XPATH,\"//div[@jsname='i3y3Ic']/input\").click()    \n",
    "            except:\n",
    "                break\n",
    "        last_height = new_height\n",
    "\n",
    "    #download images from web\n",
    "    for i in range(len(src)):\n",
    "        try:\n",
    "            urllib.request.urlretrieve(str(src[i]),f\"{folder_path}/img{i}.jpg\")\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    print('Images Downloaded: ', len(src))\n",
    "    \n",
    "#print error\n",
    "except Exception as e:\n",
    "    print('error', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "847ca757",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "036f3100",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fba24c9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6695a32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
